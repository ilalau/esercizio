{
  "metadata": {
    "toc-autonumbering": true,
    "toc-showmarkdowntxt": false,
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "For the resoultion of the 1st exercise I made these assumptions:\n\n-The start and end times for the input time interval are provided as string inputs in the format 'YYYY-MM-DDTHH:MM:SS'.\n-Faulty intervals refer to specific time ranges during which the metrics data is not reliable.\n-We don't have any info related to the different kitchens.We don't have any kitchen column, or kitchen value. \nI don't want to assume that we don't have enough info to solve the problem (since we cannot filter for K1), so let's assume that the k1 is the combination of a1 and m1. \n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\ndef prepare_training_dataset(start_time, end_time):\n    # Load the cooking metrics dataset\n   # Load the datasets\n    cooking_metrics = pd.read_csv('cooking_metrics.csv', delimiter=\";\", engine='python')\n    batch_registry = pd.read_csv('batch_registry.csv', delimiter=\";\", engine='python')\n    faulty_intervals = pd.read_csv('faulty_intervals.csv', delimiter=\";\", engine='python')\n\n    # Filter data for machine m1 \n    filtered_cooking_metrics = cooking_metrics[\n        (cooking_metrics['machine_id'] == 'm1')\n    ]\n\n    # Filter data based on the specified time interval \n    filtered_cooking_metrics = filtered_cooking_metrics[\n        (filtered_cooking_metrics['timestamp'] >= start_time) &\n        (filtered_cooking_metrics['timestamp'] <= end_time)\n    ]\n    \n    faulty_intervals = faulty_intervals[faulty_intervals['machine_id'] == 'm1']\n    batch_registry = batch_registry[batch_registry['arepa_type'] == 'a1']\n\n    # Remove faulty intervals that can be found in the corrispective csv\n    for _, row in faulty_intervals.iterrows():\n        faulty_start = row['start_time']\n        faulty_end = row['end_time']\n        filtered_cooking_metrics = filtered_cooking_metrics[\n            ~((filtered_cooking_metrics['timestamp'] >= faulty_start) &\n              (filtered_cooking_metrics['timestamp'] <= faulty_end))\n        ]\n\n    # Calculate hourly averaged metrics\n    filtered_cooking_metrics['timestamp'] = pd.to_datetime(filtered_cooking_metrics['timestamp'])\n    #hour_intervals = pd.date_range(start=start_date, end=end_date, freq='H')\n    #hourly_averaged_metrics = filtered_cooking_metrics.resample('1H', on='timestamp').mean().reset_index()\n\n    # Merge with batch registry to include batch information based on shared key\n    training_dataset = pd.merge(filtered_cooking_metrics, batch_registry, on=['batch_id'])\n    training_dataset['metric_1'] =  training_dataset['metric_1'].str.replace(',', '.').astype(float)\n    training_dataset['metric_2'] =  training_dataset['metric_2'].str.replace(',', '.').astype(float)\n\n    # Calculate the average between two metric values for the timestamp\n    training_dataset['Average'] = (training_dataset['metric_1'] + training_dataset['metric_2']) / 2\n    \n    training_dataset.drop(['batch_id', 'metric_1','metric_2'], axis=1,inplace=True)\n    \n    return training_dataset",
      "metadata": {
        "trusted": true
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This code defines a function called prepare_training_dataset that takes the start and end times as input and returns the training dataset with hourly averaged metrics for the specified combination of machine m1 and arepa type a1 in a given time interval. \nThe faulty intervals are filtered out before calculating the hourly averages, with the input data we have, there's no way to divide per hour.\n\nTo use the function, you can call it with the desired start and end times:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "start_time = '2020-07-01T00:00:00'\nend_time = '2023-07-02T00:00:00'\ntraining_dataset = prepare_training_dataset(start_time, end_time)\nprint(training_dataset)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "text": "            timestamp machine_id arepa_type  Average\n0 2020-11-01 01:30:00         m1         a1  0.47085\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "To scale the training to consider all machines in all kitchens associated with phase 1, but with a reduced list of arepa types provided in a text file, the code would need to be modified. \n\nBelow are the required changes:\n\nLoad the list of arepa types from the text file: \n-Add an additional input parameter to the prepare_training_dataset function to accept the location of the text file containing the list of arepa types.\n-Read the text file to extract the list of arepa types.\n\nModify the filtering step for arepa types:\n-Instead of filtering based on a single arepa type, we have to use the list of arepa types obtained from the text file.\n-Update the filtering condition to check if the arepa type is present in the list.\n    ",
      "metadata": {}
    }
  ]
}